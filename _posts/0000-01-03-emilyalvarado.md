---
layout: slide
title: "Crawler Based Search Engines"

---

<body> 
## Crawler-based search engines create their databases or lists automatically without any human intervention.
Search engines operate in three steps:
    1. Web crawling: A web crawler, also known as a web spider or bot, is a computer program or software specifically designed to collect and store data about websites for indexing.
   2. Indexing: Indexing helps classify a site correctly for searching purposes. The data crawled or extracted is then indexed and stored in a database for quick access.
   3. Searching: The final step in search engine operations. When a user requests specific information by entering keywords in a search engine, the search engine queries the index and provides a list of the most relevant webpages by matching it with the indexed keywords.
</body>
